{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCAA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunflowersunflower/NCAA/blob/master/NCAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6dmEAX1wk15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "import random\n",
        "import numpy\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIL0YE7FxgoC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title calculate elo score \n",
        "def calc_elo(win_team, lose_team, season):\n",
        "    winner_rank = get_elo(season, win_team)\n",
        "    loser_rank = get_elo(season, lose_team)\n",
        "    rank_diff = winner_rank - loser_rank\n",
        "    exp = (rank_diff * -1) / 400\n",
        "    odds = 1 / (1 + math.pow(10, exp))\n",
        "    if winner_rank < 2100:\n",
        "        k = 32\n",
        "    elif winner_rank >= 2100 and winner_rank < 2400:\n",
        "        k = 24\n",
        "    else:\n",
        "        k = 16\n",
        "    new_winner_rank = round(winner_rank + (k * (1 - odds)))\n",
        "    new_rank_diff = new_winner_rank - winner_rank\n",
        "    new_loser_rank = loser_rank - new_rank_diff\n",
        "\n",
        "    return new_winner_rank, new_loser_rank\n",
        "  \n",
        "  \n",
        "def get_elo(season, team):\n",
        "    try:\n",
        "        return team_elos[season][team]\n",
        "    except:\n",
        "        try:\n",
        "            # Get the previous season's ending value.\n",
        "            team_elos[season][team] = team_elos[season-1][team]\n",
        "            return team_elos[season][team]\n",
        "        except:\n",
        "            # Get the starter elo.\n",
        "            team_elos[season][team] = base_elo\n",
        "            return team_elos[season][team]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEmNxb2DyXtc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title build data \n",
        "\n",
        "def initialize_data():\n",
        "    for i in range(1985, prediction_year+1):\n",
        "        team_elos[i] = {}\n",
        "        team_stats[i] = {}\n",
        "\n",
        "def update_stats(season, team, fields):\n",
        "    if team not in team_stats[season]:\n",
        "        team_stats[season][team] = {}\n",
        "\n",
        "    for key, value in fields.items():\n",
        "        # Make sure we have the field.\n",
        "        if key not in team_stats[season][team]:\n",
        "            team_stats[season][team][key] = []\n",
        "        if len(team_stats[season][team][key]) >= 9:\n",
        "            team_stats[season][team][key].pop()\n",
        "        team_stats[season][team][key].append(value)\n",
        "        \n",
        "def get_stat(season, team, field):\n",
        "    try:\n",
        "        l = team_stats[season][team][field]\n",
        "        return sum(l) / float(len(l))\n",
        "    except:\n",
        "        return 0\n",
        "      \n",
        "def build_team_dict():\n",
        "    team_ids = pd.read_csv(folder + '/Teams.csv')\n",
        "    team_id_map = {}\n",
        "    for index, row in team_ids.iterrows():\n",
        "        team_id_map[row['Team_Id']] = row['Team_Name']\n",
        "    return team_id_map\n",
        "  \n",
        "def build_season_data(all_data):\n",
        "    # Calculate the elo for every game for every team, each season.\n",
        "    # Store the elo per season so we can retrieve their end elo\n",
        "    # later in order to predict the tournaments without having to\n",
        "    # inject the prediction into this loop.\n",
        "    print(\"Building season data.\")\n",
        "    for index, row in all_data.iterrows():\n",
        "        # Used to skip matchups where we don't have usable stats yet.\n",
        "        skip = 0\n",
        "\n",
        "        # Get starter or previous elos.\n",
        "        team_1_elo = get_elo(row['Season'], row['Wteam'])\n",
        "        team_2_elo = get_elo(row['Season'], row['Lteam'])\n",
        "\n",
        "        if row['Wloc'] == 'H':\n",
        "            team_1_elo += 100\n",
        "        elif row['Wloc'] == 'A':\n",
        "            team_2_elo += 100\n",
        "\n",
        "        team_1_features = [team_1_elo]\n",
        "        team_2_features = [team_2_elo]\n",
        "\n",
        "        for field in stat_fields:\n",
        "            team_1_stat = get_stat(row['Season'], row['Wteam'], field)\n",
        "            team_2_stat = get_stat(row['Season'], row['Lteam'], field)\n",
        "            if team_1_stat is not 0 and team_2_stat is not 0:\n",
        "                team_1_features.append(team_1_stat)\n",
        "                team_2_features.append(team_2_stat)\n",
        "            else:\n",
        "                skip = 1\n",
        "\n",
        "        if skip == 0:  # Make sure we have stats.\n",
        "            # Randomly select left and right and 0 or 1\n",
        "            if random.random() > 0.5:\n",
        "                X.append(team_1_features + team_2_features)\n",
        "                y.append(0)\n",
        "            else:\n",
        "                X.append(team_2_features + team_1_features)\n",
        "                y.append(1)\n",
        "\n",
        "        if row['Wfta'] != 0 and row['Lfta'] != 0:\n",
        "            stat_1_fields = {\n",
        "                'score': row['Wscore'],\n",
        "                'fgp': row['Wfgm'] / row['Wfga'] * 100,\n",
        "                'fga': row['Wfga'],\n",
        "                'fga3': row['Wfga3'],\n",
        "                '3pp': row['Wfgm3'] / row['Wfga3'] * 100,\n",
        "                'ftp': row['Wftm'] / row['Wfta'] * 100,\n",
        "                'or': row['Wor'],\n",
        "                'dr': row['Wdr'],\n",
        "                'ast': row['Wast'],\n",
        "                'to': row['Wto'],\n",
        "                'stl': row['Wstl'],\n",
        "                'blk': row['Wblk'],\n",
        "                'pf': row['Wpf']\n",
        "            }\n",
        "            stat_2_fields = {\n",
        "                'score': row['Lscore'],\n",
        "                'fgp': row['Lfgm'] / row['Lfga'] * 100,\n",
        "                'fga': row['Lfga'],\n",
        "                'fga3': row['Lfga3'],\n",
        "                '3pp': row['Lfgm3'] / row['Lfga3'] * 100,\n",
        "                'ftp': row['Lftm'] / row['Lfta'] * 100,\n",
        "                'or': row['Lor'],\n",
        "                'dr': row['Ldr'],\n",
        "                'ast': row['Last'],\n",
        "                'to': row['Lto'],\n",
        "                'stl': row['Lstl'],\n",
        "                'blk': row['Lblk'],\n",
        "                'pf': row['Lpf']\n",
        "            }\n",
        "            update_stats(row['Season'], row['Wteam'], stat_1_fields)\n",
        "            update_stats(row['Season'], row['Lteam'], stat_2_fields)\n",
        "\n",
        "        # Now that we've added them, calc the new elo.\n",
        "        new_winner_rank, new_loser_rank = calc_elo(\n",
        "            row['Wteam'], row['Lteam'], row['Season'])\n",
        "        team_elos[row['Season']][row['Wteam']] = new_winner_rank\n",
        "        team_elos[row['Season']][row['Lteam']] = new_loser_rank\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PUfpfa2yGTk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title prediction of result \n",
        "def predict_winner(team_1, team_2, model, season, stat_fields):\n",
        "    features = []\n",
        "\n",
        "    # Team 1\n",
        "    features.append(get_elo(season, team_1))\n",
        "    for stat in stat_fields:\n",
        "        features.append(get_stat(season, team_1, stat))\n",
        "\n",
        "    # Team 2\n",
        "    features.append(get_elo(season, team_2))\n",
        "    for stat in stat_fields:\n",
        "        features.append(get_stat(season, team_2, stat))\n",
        "\n",
        "    return model.predict_proba([features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EduPAJ9ztJx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title helper function for printing out grid search results \n",
        "def print_grid_search_metrics(gs):\n",
        "    print (\"Best score: %0.3f\" % gs.best_score_)\n",
        "    print (\"Best parameters set:\")\n",
        "    best_parameters = gs.best_params_\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHw9ytRj0JVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title load data \n",
        "file_id='1PVdFEP4YXHdQ8vc-Vi75roecxhcBc1D1'\n",
        "link='https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "csv_url=link.format(FILE_ID=file_id)\n",
        "season_data = pd.read_csv(csv_url)\n",
        "\n",
        "file_id='11w8bD8Pm1LMIvc7Cu3V63FFxjfEP5Jd0'\n",
        "link='https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "csv_url=link.format(FILE_ID=file_id)\n",
        "tourney_data = pd.read_csv(csv_url)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAFLlJxW1JYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_elo = 1600\n",
        "team_elos = {}  \n",
        "team_stats = {}\n",
        "X = []\n",
        "y = []\n",
        "prediction_year = 2016"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyGkgVeOyAjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    stat_fields = ['score', 'fga', 'fgp', 'fga3', '3pp', 'ftp', 'or', 'dr',\n",
        "                   'ast', 'to', 'stl', 'blk', 'pf']\n",
        "    initialize_data()\n",
        "    frames = [season_data, tourney_data]\n",
        "    all_data = pd.concat(frames)\n",
        "\n",
        "    # Build the working data.\n",
        "    X, y = build_season_data(all_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT-gTpWFx2Pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = pd.DataFrame(X, columns = ['Elo'] + stat_fields + ['Elo'] + stat_fields)\n",
        "y_data = pd.DataFrame(y, columns = ['win'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW7KN8yS6AlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mvz6dap4tno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = X_data.corr()\n",
        "sns.heatmap(corr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgug7KxY6Vg0",
        "colab_type": "text"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTtQ3OM_646z",
        "colab_type": "text"
      },
      "source": [
        "Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EScOv-UK6Up8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_data, y_data, test_size=0.2)\n",
        "\n",
        "print('training data has %d observation with %d features'% X_train.shape)\n",
        "print('test data has %d observation with %d features'% X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlPZguNY6h-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale the data, using standardization\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrzPI6b6_ed",
        "colab_type": "text"
      },
      "source": [
        "Model Training and Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZTdB8fV680D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title build models\n",
        "\n",
        "# Logistic Regression\n",
        "classifier_logistic = LogisticRegression()\n",
        "\n",
        "# K Nearest Neighbors\n",
        "classifier_KNN = KNeighborsClassifier()\n",
        "\n",
        "# Random Forest\n",
        "classifier_RF = RandomForestClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU4hSZ1o6sbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "classifier_logistic.fit(X_train, y_train)\n",
        "# Prediction of test data\n",
        "classifier_logistic.predict(X_test)\n",
        "# Accuracy of test data\n",
        "classifier_logistic.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GupPHIrX5nal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use 5-fold Cross Validation to get the accuracy for different models\n",
        "model_names = ['Logistic Regression','KNN','Random Forest']\n",
        "model_list = [classifier_logistic, classifier_KNN, classifier_RF]\n",
        "count = 0\n",
        "\n",
        "for classifier in model_list:\n",
        "    cv_score = model_selection.cross_val_score(classifier, numpy.array(X), numpy.array(y), cv=5)\n",
        "    # cprint(cv_score)\n",
        "    print('Model accuracy of %s is: %.3f'%(model_names[count],cv_score.mean()))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLCE_F7f8rou",
        "colab_type": "text"
      },
      "source": [
        "Find Optimal Hyperparameters - LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcEej9wr73is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Logistic Regression Regularization\n",
        "# Penalty is choosed from L1 or L2\n",
        "# C is the lambda value(weight) for L1 and L2\n",
        "parameters = {\n",
        "    'penalty':('l1', 'l2'), \n",
        "    'C':(0.2, 0.4, 0.8, 1.6, 3.2, 5)\n",
        "}\n",
        "Grid_LR = GridSearchCV(LogisticRegression(),parameters, cv=5)\n",
        "Grid_LR.fit(numpy.array(X), numpy.array(y))\n",
        "# the best hyperparameter combination\n",
        "print_grid_search_metrics(Grid_LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFn7T9ja9wzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "best_LR_model = Grid_LR.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6m83IGy8tTn",
        "colab_type": "text"
      },
      "source": [
        "Find Optimal Hyperparameters: KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjtaMxyU8xvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for KNN\n",
        "# Choose k\n",
        "parameters = {\n",
        "    'n_neighbors':[3,5,7,10] \n",
        "}\n",
        "Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=5)\n",
        "Grid_KNN.fit(X_train, y_train)\n",
        "print_grid_search_metrics(Grid_KNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybEdMTn885Tm",
        "colab_type": "text"
      },
      "source": [
        " Find Optimal Hyperparameters: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob7pMfkd88AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Random Forest\n",
        "# Choose the number of trees\n",
        "parameters = {\n",
        "    'n_estimators' : [40,60,80]\n",
        "}\n",
        "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
        "Grid_RF.fit(X_train, y_train)\n",
        "# best number of tress\n",
        "print_grid_search_metrics(Grid_RF)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyjTKSqd9zNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best random forest\n",
        "best_RF_model = Grid_RF.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zba2fLZe9C2l",
        "colab_type": "text"
      },
      "source": [
        "Model Evaluation - Confusion Matrix (Precision, Recall, Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I1orJ3c9FqF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title helper function for confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# calculate accuracy, precision and recall\n",
        "def cal_evaluation(classifier, cm):\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    tp = cm[1][1]\n",
        "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
        "    precision = tp / (tp + fp + 0.0)\n",
        "    recall = tp / (tp + fn + 0.0)\n",
        "    print (classifier)\n",
        "    print (\"Accuracy is: %0.3f\" % accuracy)\n",
        "    print (\"precision is: %0.3f\" % precision)\n",
        "    print (\"recall is: %0.3f\" % recall)\n",
        "\n",
        "# print out confusion matrices\n",
        "def draw_confusion_matrices(confusion_matricies):\n",
        "    class_names = ['Not','Churn']\n",
        "    for cm in confusion_matrices:\n",
        "        classifier, cm = cm[0], cm[1]\n",
        "        cal_evaluation(classifier, cm)\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
        "        plt.title('Confusion matrix for %s' % classifier)\n",
        "        fig.colorbar(cax)\n",
        "        ax.set_xticklabels([''] + class_names)\n",
        "        ax.set_yticklabels([''] + class_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXXkzD9TM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion matrix, accuracy, precison and recall for random forest and logistic regression\n",
        "confusion_matrices = [\n",
        "    (\"Random Forest\", confusion_matrix(y_test,best_RF_model.predict(X_test))),\n",
        "    (\"Logistic Regression\", confusion_matrix(y_test,best_LR_model.predict(X_test))),\n",
        "]\n",
        "\n",
        "draw_confusion_matrices(confusion_matrices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oidozQtf991H",
        "colab_type": "text"
      },
      "source": [
        "Model Evaluation - ROC & AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45LVYEVQ9-8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn import metrics\n",
        "\n",
        "# Use predict_proba to get the probability results of Random Forest\n",
        "y_pred_rf = best_RF_model.predict_proba(X_test)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W80AG8Oc-E9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ROC curve of Random Forest result\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - RF model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjxuGgcU-Io3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUC score\n",
        "metrics.auc(fpr_rf,tpr_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDK5uzP-MwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use predict_proba to get the probability results of Logistic Regression\n",
        "y_pred_lr = best_LR_model.predict_proba(X_test)[:, 1]\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUTMPTAs-OqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ROC Curve\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_lr, tpr_lr, label='LR')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - LR Model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtQJDKbF-Q_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUC score\n",
        "metrics.auc(fpr_lr,tpr_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHoFyehT-b1g",
        "colab_type": "text"
      },
      "source": [
        "Random Forest Model - Feature Importance Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwsEKUl_-Y-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check feature importance of random forest for feature selection\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X_data, y_data)\n",
        "\n",
        "importances = forest.feature_importances_\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature importance ranking by Random Forest Model:\")    #round(number, number of digits)\n",
        "for k,v in sorted(zip(map(lambda x: round(x, 4), importances), X_data.columns), reverse=True):\n",
        "    print (v + \": \" + str(k))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o94drlURdy9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "  plt.figure()\n",
        "  plt.title(title)\n",
        "  if ylim is not None:\n",
        "      plt.ylim(*ylim)\n",
        "  plt.xlabel(\"Training examples\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "  train_scores_mean = np.mean(train_scores, axis=1)\n",
        "  train_scores_std = np.std(train_scores, axis=1)\n",
        "  test_scores_mean = np.mean(test_scores, axis=1)\n",
        "  test_scores_std = np.std(test_scores, axis=1)\n",
        "  plt.grid()\n",
        "\n",
        "  plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "  plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "  plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "  plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "           label=\"Cross-validation score\")\n",
        "\n",
        "  plt.legend(loc=\"best\")\n",
        "  return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wUzRQWHeIWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "title = \"Learning Curves (Naive Bayes)\"\n",
        "# Cross validation with 100 iterations to get smoother mean test and train\n",
        "# score curves, each time with 20% data randomly selected as a validation set.\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
        "\n",
        "estimator = GaussianNB()\n",
        "plot_learning_curve(estimator, title, X_data, y_data, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator = SVC(gamma=0.001)\n",
        "plot_learning_curve(estimator, title, X_data, y_data, (0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}